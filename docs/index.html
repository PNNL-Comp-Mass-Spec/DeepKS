<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>Table of Contents</title>
        <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css">
<link href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css" rel="stylesheet" type="text/css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
        
    </head>
    <body class="vscode-body vscode-light">
        <style>
    :root{
        --pctg: 25%;
    }
    iframe {
        border-width: 2px;
        border-color: black;
        border-style: solid;
        border-radius: 6px;
    }
    pre.bash-output.bash-output{
        background-color: #ebe9c27f;
    }
    code.inline-bash-output{
        background-color: #ebe9c27f;
    }
    code{
        background-color: rgba(220, 220, 220, 0.5);
        padding: 1px 3px;
        border-radius: 5px;
    }
    pre code{
        background-color: transparent;
        padding: 0;
        border-radius: 0;
    }
    h1{
        border-bottom-width: 2px;
        margin-top: 50px;
        margin-bottom:10px;
        margin-left:0px;
        margin-right:0px;
    }
    h1:first-of-type{
        border-bottom-width: 2px;
        margin-top: 10px;
        margin-bottom:10px;
        margin-left:0px;
        margin-right:0px;
    }
    
    h2{
        border-bottom-width: 1px;
        border-bottom-color: #00000040;
        border-bottom-style: solid;
        margin-top: 25px;
    }

    h3{
        border-bottom-width: 1px;
        border-bottom-color: #00000040;
        border-bottom-style: dashed;
        margin-top: 25px;
    }

    h4, h5, h6{
        margin-top: 25px;
    }

    /* .tab-cell {
        vertical-align: top;
    } */

    .tab-cell-inner {
        border-radius: 3px;
        border-style: solid;
        border-width: 1.5px;
        padding: 12px;
        margin-top: 12px;
        margin-bottom: 12px;
    }

    html,
    body {
        padding: unset;
        margin: unset;
        border: unset;
        background: unset;
    }
</style>
<div class="tab-cont">
<div class="tab-cell" style="float:left; width:var(--pctg);">
<div class="tab-cell-inner" style="margin-left:12px; margin-right:6px;">
<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#quickstart-gets-things-up-and-running-but-does-not-explain-the-tool--the-rest-of-the-manual-goes-in-depth">Quickstart (Gets things up and running, but does not explain the tool — the rest of the manual goes in depth)</a></li>
<li><a href="#colors-in-this-manual">Colors in this manual</a></li>
<li><a href="#general-notes-relating-to-devices-read-before-running-any-program">General Notes Relating to Devices (Read before running any program)</a>
<ul>
<li><a href="#does-my-computer-have-a-cuda-compatible-gpu">Does My Computer Have a CUDA-compatible GPU?</a></li>
<li><a href="#follow-one-of-the-cases-below">Follow <strong>one</strong> of the cases below.</a>
<ul>
<li><a href="#case-a-running-on-personal-computer-with-cuda">Case A: Running On Personal Computer with CUDA</a></li>
<li><a href="#case-b-running-on-personal-computer-without-cuda">Case B: Running On Personal Computer without CUDA</a></li>
<li><a href="#case-c-running-on-hpc-cluster">Case C: Running On HPC Cluster</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#getting-started-with-docker">Getting Started with Docker</a>
<ul>
<li><a href="#follow-one-of-the-cases-below-1">Follow <strong>one</strong> of the cases below.</a>
<ul>
<li><a href="#case-ab-running-on-personal-computer">Case A/B: Running On Personal Computer</a></li>
<li><a href="#case-c-running-on-hpc-cluster-1">Case C: Running on HPC cluster</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#running-the-programs-and-api">Running The Programs and API</a>
<ul>
<li><a href="#using-api">Using API</a>
<ul>
<li><a href="#from-command-line">From Command Line</a></li>
<li><a href="#adding-to-new-code-and-importing">Adding to new code and importing</a>
<ul>
<li><a href="#vs-code-integration">VS Code Integration</a></li>
<li><a href="#importing">Importing</a></li>
</ul>
</li>
<li><a href="#full-api-specification">Full API Specification</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#file-explainer">File Explainer</a></li>
<li><a href="#reproducing-everything-from-scratch">Reproducing Everything From Scratch</a>
<ul>
<li><a href="#preprocessing-and-data-collection">Preprocessing and Data Collection</a></li>
<li><a href="#training">Training</a></li>
<li><a href="#evaluating">Evaluating</a></li>
<li><a href="#creating-evaluation-diagrams">Creating Evaluation Diagrams</a></li>
<li><a href="#creating-other-diagrams">Creating Other Diagrams</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="tab-cell" style="float:right; width:calc(100% - var(--pctg));">
<div class="tab-cell-inner" style="margin-right:12px; margin-left:6px;">
<h1 style='font-size:36pt'>DeepKS Manual</h1>
<p><span style='font-size:15pt'> The bulk of the DeepKS tool is run through Docker. It will essentially run like it would in a virtual machine. This makes dependency management a breeze and ensures the program will run exactly the same way on every computer. Follow the steps below to get started. One need not clone the DeepKS Git repository to use the tool. </span></p>
<h1 id="quickstart-gets-things-up-and-running-but-does-not-explain-the-tool--the-rest-of-the-manual-goes-in-depth">Quickstart (Gets things up and running, but does not explain the tool — the rest of the manual goes in depth)</h1>
<ol>
<li>See the <a href="https://ben-drucker.gitlab.io/deepks-rename-trial/doc/quickstart.html">Quickstart guide</a>.</li>
</ol>
<h1 id="colors-in-this-manual">Colors in this manual</h1>
<ul>
<li><code>This style is used for code the user should run.</code></li>
<li><code class = "inline-bash-output">This style is used to represent the desired output of a command.</code></li>
</ul>
<h1 id="general-notes-relating-to-devices-read-before-running-any-program">General Notes Relating to Devices (Read before running any program)</h1>
<h2 id="does-my-computer-have-a-cuda-compatible-gpu">Does My Computer Have a CUDA-compatible GPU?</h2>
<p>If you're not sure, follow the instructions <a href="https://askubuntu.com/a/1273434">here</a>.</p>
<h2 id="follow-one-of-the-cases-below">Follow <strong>one</strong> of the cases below.</h2>
<h3 id="case-a-running-on-personal-computer-with-cuda">Case A: Running On Personal Computer with CUDA</h3>
<p>If you have a CUDA-compatible GPU, you can run the program on your personal computer and take advantage of the GPU. This is the fastest way to run the program (besides using an HPC cluster).</p>
<p>Most likely, your computer will be running Windows. If this is the case, there is some additional setup involved. If you want to bypass this setup, you can run the program without CUDA on your personal computer or on a HPC cluster (see below). But if you do want to run the program with CUDA on your personal computer, do the following:</p>
<ol>
<li>Go through the steps in the <a href="https://ben-drucker.gitlab.io/deepks-rename-trial/doc/cuda_installation.html">auxillary help page</a>.</li>
</ol>
<h3 id="case-b-running-on-personal-computer-without-cuda">Case B: Running On Personal Computer without CUDA</h3>
<ol>
<li>Download Docker here <a href="https://www.docker.com/products/docker-desktop/">https://www.docker.com/products/docker-desktop/</a> and follow the installation instructions for your operating system.</li>
</ol>
<h3 id="case-c-running-on-hpc-cluster">Case C: Running On HPC Cluster</h3>
<p><em><strong>Note: These instructions are specific to the PNNL &quot;deception&quot; cluster (it assumes <code>module</code> and <code>apptainer</code> are preinstalled and configured appropriately). It also assumes you have an active account.</strong></em></p>
<ol start="0">
<li>Make sure you are connected to the cluster's VPN. (In the case of PNNL, make sure you are on campus or connected to the Onekey VPN.)</li>
<li>Open a terminal SSH into the cluster with <code>ssh &lt;username&gt;@deception</code>, making sure to replace <code>&lt;username&gt;</code> with your actual username.</li>
<li>Download the interactive Slurm script by running <code>cd ~ &amp;&amp; wget https://gitlab.com/Ben-Drucker/deepks/-/raw/main/hpc/.interactive_slurm_script.py?inline=false</code></li>
<li>Run <code>python .interactive_slurm_script.py</code>. This will request an interactive session on a compute node.</li>
<li>Ensure your session is loaded (i.e., that you are now in a terminal on the HPC. You can check this by running <code>hostname</code>. It should no longer be <code>deception0X</code>.)</li>
<li>Run <code>module load apptainer</code> to load Apptainer.</li>
</ol>
<h1 id="getting-started-with-docker">Getting Started with Docker</h1>
<h2 id="terminology"> Terminology </h2>
Please read this explanation: "[An image is a blueprint for a snapshot of a 'system-in-a-system' (similar to a virtual machine).] An instance of an image is called a container...If you start this image, you have a running container of this image. You can have many running containers of the same image." ~ <a href="https://stackoverflow.com/a/23736802/16158339">Thomas Uhrig and Alex Telon's post</a>
<h2 id="follow-one-of-the-cases-below">Follow <strong>one</strong> of the cases below.</h2>
<h3 id="case-ab-running-on-personal-computer">Case A/B: Running On Personal Computer</h3>
<ol>
<li>If using WSL, make sure it is running. Otherwise, ensure Docker Desktop (Installed above) is running and a terminal is open.</li>
<li>Run the following command to start the docker session: <code>docker run -it --name deepks-container --network host --hostname deepks-container benndrucker/deepks</code>.
<ol>
<li>The name <code>deepks-container</code> is arbitrary. You can name it whatever you want. In fact, if you need to run multiple instances of the Docker container, you must name them differently.</li>
</ol>
</li>
<li>The interface — in an attempt to update the git repository, will ask for your username and password. Fill that in.</li>
<li>A command prompt should appear and look like <code class = "inline-bash-output">(base) //root@deepks-container// [/] ▷ </code>. You are now inside the Docker Container at the top-level <code>/</code> directory. See the steps below to run various programs <em>from this prompt</em>.</li>
<li>To reuse the created container (so that any saved state is available), run <code>docker ps -a</code>. This will show a list of all running and previously-created containers.</li>
<li>Note the name of the container you want to start.</li>
<li>Run <code>docker container start &lt;noted name&gt; -i</code> (making sure to replace <code>&lt;noted name&gt;</code> with the container name you noted). This will give you the command prompt inside the Docker container.</li>
</ol>
<h3 id="case-c-running-on-hpc-cluster">Case C: Running on HPC cluster</h3>
<p>Because we will use Apptainer to run the docker container, the commands are different from cases A/B.</p>
<ol>
<li>Ensure Apptainer is loaded (<code>module load apptainer</code>).</li>
<li>Run <code>apptainer build --sandbox deepks-latest.sif docker://benndrucker/deepks:latest</code> to build the Apptainer-compatible <code>.sif</code> directory. This will take a while (~30-60+ mins) depending on your internet connection and processor speed. You may get <code>xattr</code>-related warnings, but these are fine.</li>
<li>Copy necessary Nvidia files using the following script (ensuring you are in the same directory as <code>deepks-latest.sif</code>):</li>
</ol>
<pre><code class="language-{bash}">cp /usr/bin/nvidia-smi deepks-latest.sif/usr/bin/
cp /usr/bin/nvidia-debugdump deepks-latest.sif/usr/bin/
cp /usr/bin/nvidia-persistence deepks-latest.sif/usr/bin/
cp /usr/bin/nvidia-cuda-mps-server deepks-latest.sif/usr/bin/
cp /usr/bin/nvidia-cuda-mps-control deepks-latest.sif/usr/bin/
</code></pre>
<ol start="4">
<li>The top-level directory structure of <code>deepks-latest.sif</code> must mirror the native root directory. Thus, when running the next command, you may receive binding or mounting errors. The solution is creating &quot;fake,&quot; empty directories at the top level of <code>deepks-latest.sif</code>. [PNNL SPECIFIC] You need to make the &quot;fake&quot; directory <code>/people</code> in <code>deepks-latest.sif</code> by running <code>mkdir deepks-latest.sif/people</code>.</li>
<li>Run <code>apptainer shell --nv --writable --fakeroot deepks-latest.sif</code> to start the Docker container (in Apptainer). This may give three warnings about Nvidia and mounts:</li>
</ol>
<pre class = "bash-output bash-output">
<span style="color:#AAA956">WARNING:</span> nv files may not be bound with --writable
<span style="color:#AAA956">WARNING:</span> Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container
<span style="color:#AAA956">WARNING:</span> Skipping mount /var/run/nvidia-persistenced/socket [files]: /var/run/nvidia-persistenced/socket doesn't exist in container
</pre>
<p>These don't seem to cause any issues.</p>
<ol start="6">
<li>Change directory to <code>/</code> (i.e., the top-level directory) by running <code>cd /</code>.</li>
</ol>
<p><em><strong>Note: You will have <code>sudo</code> privileges inside the Docker container (!) by virtue of passing <code>--fakeroot</code>. If you ever need to install programs, for example, this means you can do so inside the container.</strong></em></p>
<h1 id="running-the-programs-and-api">Running The Programs and API</h1>
<p><em><strong>Note: The following steps are run from <u> inside the Docker container</u>. See the steps above to start the Docker container.</strong></em></p>
<h2 id="using-api">Using API</h2>
<h3 id="from-command-line">From Command Line</h3>
<p>The Command Line Interface is the main way to query the deep learning model. The API is a submodule of <code>DeepKS</code>. (Henceforth referred to as <code>DeepKS.api</code>.) The <code>DeepKS.api</code> module, itself contains a submodule <code>main</code>. (Henceforth referred to as <code>DeepKS.api.main</code>). This is the main &quot;entrypoint&quot; for running queries. Because of various Python specifications, <code>DeepKS.api.main</code> must be run as a module from <em>outside</em> the <code>/DeepKS</code> directory. Hence, to run from the command line in the Docker container, run</p>
<pre><code class="language-bash"><span class="hljs-built_in">cd</span> /
python -m DeepKS.api.main [options]
</code></pre>
<p>where <code>[options]</code> are the options you wish to pass to the program. To see the required and optional arguments, run <code>python -m DeepKS.api.main --help</code>.</p>
<p>When you run this, it will print</p>
<pre><code class="language-bash">usage: python -m DeepKS.api.main [-h] (-k &lt;kinase sequences&gt; | -kf &lt;kinase sequences file&gt;)
                                 (-s &lt;site sequences&gt; | -sf &lt;site sequences file&gt;)
                                 [--kin-info &lt;kinase info file&gt;] [--site-info &lt;site info file&gt;]
                                 [--cartesian-product]
                                 [-p {inorder,dictionary,in_order_json,dictionary_json,csv,sqlite}]
                                 [--suppress-seqs-in-output] [-v]
                                 [--pre_trained_nn &lt;pre-trained neural network file&gt;]
                                 [--pre_trained_gc &lt;pre-trained group classifier file&gt;]
                                 [--device &lt;device&gt;] [--scores] [--normalize-scores] [--<span class="hljs-built_in">groups</span>]
                                 [--dry-run]
</code></pre>
<ul>
<li>Anything in square brackets is optional and has default values. To view what these flags refer to (and their default values), run <code>python -m DeepKS.api.main --help</code>.</li>
<li>For each instance of round parentheses, you must provide one of the options between &quot;<code>|</code>&quot;.</li>
<li>Curly braces show available options for a flag.</li>
<li>That is, as a minimal example, you may run <code>python -m DeepKS.api.main -kf my_kinase_sequences.txt -sf my_site_sequences.txt</code>.</li>
<li>Maximally, you might run <code>python3 -m DeepKS.api.main -kf my_kinase_sequences.txt -sf my_site_sequences.txt --kin-info my_kinase_info.txt --site-info my_site_info.txt --cartesian-product -p in_order_json -v --pre_trained_nn my_pre_trained_nn.pt --pre_trained_gc my_pre_trained_gc.pt --device cuda:0 --scores --normalize-scores --groups --dry-run</code>.</li>
</ul>
<p><em><strong>Note: If using CUDA, it may be helpful to run <code>nvidia-smi</code> to see which GPUs is being used extensively. DeepKS can automatically scale for the available hardware, but it will run much faster if it is run on a GPU with no other concurrent processes.</strong></em></p>
<p>Here are some more examples of how to run the program (make sure to be in the top-level <code>/</code> directory):</p>
<pre><code class="language-bash">python -m DeepKS.api.main -kf my/kinase/sequences.txt -sf my/site/sequences.txt -p in_order_json -v True --device cuda:4

python -m DeepKS.api.main -k KINASE_SEQ_1,KINASE_SEQ_2,KINASE_SEQ_3 -s SITE_SEQ_1,SITE_SEQ_2,SITE_SEQ_3 -p dictionary

python -m DeepKS.api.main -kf my/kinase/sequences.txt -s SITE_SEQ_1,SITE_SEQ_2,SITE_SEQ_3 -p inorder -v False

python -m DeepKS.api.main -kf my/kinase/sequences.txt -sf my/site/sequences.txt --dry-run
</code></pre>
<p><em><strong>Note: The example files above are for example purposes only and don't actually exist.</strong></em></p>
<h3 id="adding-to-new-code-and-importing">Adding to new code and importing</h3>
<h4 id="vs-code-integration">VS Code Integration</h4>
<p>To make adding DeepKS to an external codebase, we recommend using VS Code. To do this, follow these steps:</p>
<ol>
<li>Open VS Code.</li>
<li>Install the following extensions by searching for them:
<ul>
<li>Dev Containers</li>
<li>Remote - Tunnels</li>
<li>Remote Explorer</li>
<li>Remote - Tunnels</li>
<li>WSL (if using WSL)</li>
</ul>
</li>
<li>Go back to your terminal, and make sure you're inside the <code>benndrucker/deepks:latest</code> Docker container. Then run <code>/usr/share/code/bin/code-tunnel tunnel</code> and follow the instructions.</li>
</ol>
<h4 id="importing">Importing</h4>
<p>It is recommended to clone any external Git repositories to a directory inside the Docker container. Then, you can use VS Code to edit the files. Before importing the modules, you need to tell python where it lives. To do this, run</p>
<pre><code class="language-{bash}">printf &quot;\nexport PYTHONPATH=/parent/directory/of/cloned/git/repo/:$PYTHONPATH\n\n&quot; &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc
</code></pre>
<p>making sure to replace <code>/parent/directory/of/cloned/git/repo/</code> with the path to the directory containing the cloned Git repository.</p>
<p>To import DeepKS, you can use the following examples (depending on which module(s) you need):</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> DeepKS.api.main <span class="hljs-keyword">import</span> make_predictions
<span class="hljs-keyword">import</span> DeepKS.models.multi_stage_classifier <span class="hljs-keyword">as</span> msc
<span class="hljs-keyword">import</span> DeepKS
</code></pre>
<h3 id="full-api-specification">Full API Specification</h3>
<p>Below, you will find a scrollable list of API functions found in <code>DeepKS.api.main</code>.</p>
<!-- <div><iframe width=100% height=500px src="api_pydoctor_docs/index.html"></iframe></div> -->
<h1 id="file-explainer">File Explainer</h1>
<p>Below, you will find a scrollable tree of files in this repository and their descriptions. Boldfaced nodes represent directories.</p>
<!-- <div><iframe src="tree.html" width=100% height=500px style="overscroll-behavior:contain;"></iframe></div> -->
<h1 id="reproducing-everything-from-scratch">Reproducing Everything From Scratch</h1>
<p>TODO — still working on cleaning things up.</p>
<h2 id="preprocessing-and-data-collection">Preprocessing and Data Collection</h2>
<h2 id="training">Training</h2>
<p>The python training scripts contain command line interfaces. However, to make running easier, one can use the bash scripts in the <code>models</code> directory. The bash scripts are simply wrappers around the python scripts. The bash scripts are the recommended way to run the training scripts.</p>
<ol>
<li>Run <code>bash models/train_multi_stage_classifier.sh</code> to train the multi-stage classifier.</li>
</ol>
<h2 id="evaluating">Evaluating</h2>
<h2 id="creating-evaluation-diagrams">Creating Evaluation Diagrams</h2>
<h2 id="creating-other-diagrams">Creating Other Diagrams</h2>
</div>
</div>
</div>
<script>
document.querySelector("head > title:nth-child(2)").innerHTML = "DeepKS Manual" 
</script>
        <script async src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script>
        
    </body>
    </html>